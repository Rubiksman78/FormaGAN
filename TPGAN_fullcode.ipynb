{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wczghxT56FL"
      },
      "source": [
        "# TP GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyemBO3ISvqi"
      },
      "source": [
        "## Les imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvBgYvzTNL8g"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import models,layers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import gdown\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4flw12gySJn6"
      },
      "source": [
        "## Téléchargement du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLZNgqQ7Obze",
        "outputId": "1948e9b8-b2cd-4a1e-de09-37bb77553ef7"
      },
      "outputs": [],
      "source": [
        "#Téléchargement du dataset\n",
        "url = 'https://drive.google.com/uc?id=1F9I7iDmQ_I9Qsrav1UXlD4OiIBVSU5sl'\n",
        "output = 'dataset.tgz'\n",
        "if not os.path.exists(output):\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "#Dézippage du dataset\n",
        "\n",
        "def unzip(zip_file, dest_dir):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dest_dir)\n",
        "        \n",
        "unzip('dataset.tgz', './') #Vous pouvez changer dest_dir pour stocker le dataset dans un autre dossier, ici ce sera le dossier courant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqylv3scUY76"
      },
      "source": [
        "## Quelques paramètres généraux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFtB5YCWO2Pm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "IMG_SHAPE = (64,64,3) #A changer pour un autre dataset\n",
        "\n",
        "x_train = keras.utils.image_dataset_from_directory('dataset',image_size=(64,64),batch_size=BATCH_SIZE,shuffle=True,seed=123,labels=None)\n",
        "\n",
        "x_train = x_train.map(lambda x: (x-127.5)/127.5) #Normalisation des données entre -1 et 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "ZF6nSfKQP-IM",
        "outputId": "cba7fa66-39ee-488b-875f-78018e0ea2a1"
      },
      "outputs": [],
      "source": [
        "test_batch = next(iter(x_train))\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "for i in range(25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.axis('off')\n",
        "  #On oublie pas de faire image * 0.5 + 0.5 pour revenir dans [0,1]\n",
        "  plt.imshow(test_batch[i]*0.5+0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZZBKWamPNxO"
      },
      "source": [
        "## Le discriminateur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TasqSOiOk24X"
      },
      "outputs": [],
      "source": [
        "def define_discriminator(im_shape=(64,64,3)):\n",
        "  model = models.Sequential()\n",
        "  ###Ajouter les couches ici (rappel: on peut utiliser model.add(layers.CoucheX(arguments)))\n",
        "  model.add(layers.Conv2D(64,3,2,padding='same',input_shape=im_shape))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(128,3,2,padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(256,3,2,padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(512,3,2,padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(1,4,1,padding='valid',activation='sigmoid'))\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  return model\n",
        "\n",
        "##Petit tips : faire un .summary() pour vérifier qu'on s'est pas trompé dans les dimensions de sortie\n",
        "define_discriminator().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaDZbie7Qw2D"
      },
      "source": [
        "## Le générateur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHCUgA6lKuwj"
      },
      "outputs": [],
      "source": [
        "def define_generator(latent_dim=LATENT_DIM):\n",
        "\tmodel = models.Sequential()\n",
        "\t###Ajouter les couches ici\n",
        "\tmodel.add(layers.Dense(4*4*1024,input_shape=(latent_dim,)))\n",
        "\tmodel.add(layers.Reshape((4,4,1024)))\n",
        "\tmodel.add(layers.Conv2DTranspose(256,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2DTranspose(128,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2DTranspose(64,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2DTranspose(32,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(3,3,1,padding='same',activation='tanh'))\n",
        "\n",
        "\treturn model\n",
        "\n",
        "##Petit tips : faire un .summary() pour vérifier qu'on s'est pas trompé dans les dimensions de sortie\n",
        "define_generator().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Fbk2BnWJOO"
      },
      "source": [
        "## Le train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moZPLFqPSnz9"
      },
      "outputs": [],
      "source": [
        "def train_step(real_images,generator,discriminator,loss,g_opt,d_opt):\n",
        "  batch_size = tf.shape(real_images)[0]\n",
        "  global LATENT_DIM\n",
        "  with tf.GradientTape() as disc_tape:\n",
        "\n",
        "    ###A compléter###\n",
        "    latent_vector = tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
        "    fake_images = generator(latent_vector)\n",
        "    #Ce sont les prédictions du discriminateur\n",
        "    real_predictions = discriminator(real_images)\n",
        "    fake_predictions = discriminator(fake_images)\n",
        "\n",
        "    #Les labels sont les vrais labels des images du dataset (0 ou 1)\n",
        "    real_labels = tf.ones(shape=(batch_size,1))\n",
        "    fake_labels = tf.zeros(shape=(batch_size,1))\n",
        "\n",
        "    disc_loss_on_real = loss(real_labels,real_predictions)\n",
        "    disc_loss_on_fake = loss(fake_labels,fake_predictions)\n",
        "    disc_loss = disc_loss_on_real + disc_loss_on_fake\n",
        "    ######\n",
        "\n",
        "  disc_grad = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
        "  d_opt.apply_gradients(zip(disc_grad,discriminator.trainable_variables))\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape:\n",
        "\n",
        "    ###A compléter###\n",
        "    latent_vector = tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
        "    fake_images = generator(latent_vector)\n",
        "    fake_predictions = discriminator(fake_images)\n",
        "\n",
        "    #Rappel : on veut comparer les images générées à des 1 pour tromper le discriminateur cette fois\n",
        "    real_labels = tf.ones(shape=(batch_size,1))\n",
        "    gen_loss = loss(real_labels,fake_predictions)\n",
        "    ######\n",
        "\n",
        "  gen_grad = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
        "  g_opt.apply_gradients(zip(gen_grad,generator.trainable_variables))\n",
        "  return gen_loss,disc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPUvdewb7Ak5"
      },
      "source": [
        "## Le train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOXC7WKmTI_Z"
      },
      "outputs": [],
      "source": [
        "def train(dataset,generator,discriminator,epochs,fixed_seed = tf.random.normal((25,LATENT_DIM),seed=42)):\n",
        "  \n",
        "  ###A compléter###\n",
        "  loss = keras.losses.BinaryCrossentropy()\n",
        "  g_opt = keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
        "  d_opt = keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
        "\n",
        "  Lgen_loss = []\n",
        "  Ldisc_loss = []\n",
        "  X = []\n",
        "  j = 0\n",
        "  ######\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    progress_bar = tqdm(dataset)\n",
        "    ##Vu que c'est un dataset tensorflow, on ne peut itérer directement dessus avec son indice. On va juste prendre à chaque fois batch par batch.\n",
        "    for _,image_batch in enumerate(progress_bar):\n",
        "        j += 1\n",
        "        gen_loss, disc_loss = train_step(image_batch,generator,discriminator,loss,g_opt,d_opt)\n",
        "        \n",
        "        X.append(j)\n",
        "        Lgen_loss.append(gen_loss)\n",
        "        Ldisc_loss.append(disc_loss)\n",
        "\n",
        "    clear_output(wait=False)\n",
        "    generate_and_save_plots(X, Lgen_loss, Ldisc_loss,  epoch+1)\n",
        "    summarize_performance(generator,fixed_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSDZildZALv"
      },
      "source": [
        "## L'affichage à chaque epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D5j4ZlLXaOh"
      },
      "outputs": [],
      "source": [
        "def summarize_performance(generator,fixed_seed):\n",
        "  fake_images = generator.predict(fixed_seed)\n",
        "  fig = plt.figure(figsize=(12,12))\n",
        "  for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fake_images[i]*0.5+0.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDbwKnC1pLFN"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_plots(X, Lgen_loss, Ldisc_loss, epoch):\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    plt.plot(X,Lgen_loss, label = 'gen_loss')\n",
        "    plt.plot(X,Ldisc_loss, label = 'disc_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAx8pu5P7sOR"
      },
      "source": [
        "## Ici vous lancez tout!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "vKElP8cy8dil",
        "outputId": "4594d8f6-7c4c-40f7-9134-f054cd97a2af"
      },
      "outputs": [],
      "source": [
        "generator = define_generator(latent_dim=LATENT_DIM)\n",
        "discriminator = define_discriminator(im_shape=(64,64,3))\n",
        "EPOCHS = 10\n",
        "train(x_train,generator,discriminator,EPOCHS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TPGANatrous.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "32c9d2eacc2762f52bef3ce1121499e437c40c335720e1c9a43a308cd905b17a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
