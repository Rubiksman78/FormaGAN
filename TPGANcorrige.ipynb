{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wczghxT56FL"
      },
      "source": [
        "# TP GAN\n",
        "## Prérequis\n",
        "\n",
        "Avoir suivi la formation GAN et avoir quelques bases avec Tensorflow (juste faire un modèle .Sequential). Vous pouvez vous mettre sous les yeux le TP CNN.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "L'objectif de ce TP est de coder un GAN pour générer des petits chats trop mignons. Il abordera les notions principales vues pendant la formation GAN: les CNN, convolutions transposées, batchnorm, binary crossentropy, comment entraîner tout ce beau monde...etc (et peut-être les problèmes de GAN si ça arrive mais pas de panique).\n",
        "\n",
        "La plupart des détails sont déjà implémentés, le plus important étant d'élaborer l'architecture du générateur et du discriminateur ainsi que de comprendre comment les entraîner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyemBO3ISvqi"
      },
      "source": [
        "## Les imports\n",
        "\n",
        "On va d'abord importer les librairies nécessaires dont les classiques numpy, matplotlib ainsi que tensorflow, keras et certaines classes utilisés assez souvent (layers, models ...etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvBgYvzTNL8g"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import models,layers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import gdown\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4flw12gySJn6"
      },
      "source": [
        "## Téléchargement du dataset\n",
        "\n",
        "Importons maintenant le dataset. On va télécharget le dataset de chats depuis ce [drive](https://drive.google.com/uc?id=1F9I7iDmQ_I9Qsrav1UXlD4OiIBVSU5sl) avec la commande gdown (ou à la mano si vous préférez).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLZNgqQ7Obze",
        "outputId": "1948e9b8-b2cd-4a1e-de09-37bb77553ef7"
      },
      "outputs": [],
      "source": [
        "#Téléchargement du dataset\n",
        "url = 'https://drive.google.com/uc?id=1F9I7iDmQ_I9Qsrav1UXlD4OiIBVSU5sl'\n",
        "output = 'dataset.tgz'\n",
        "if not os.path.exists(output):\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "#Dézippage du dataset\n",
        "\n",
        "def unzip(zip_file, dest_dir):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dest_dir)\n",
        "        \n",
        "unzip('dataset.tgz', './')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqylv3scUY76"
      },
      "source": [
        "## Quelques paramètres généraux\n",
        "\n",
        "Les paramètres peuvent être modifiés pour tester un peu (sauf peut-être la taille de l'image pour ce dataset).\n",
        "\n",
        "Pour ce dataset, on peut prendre une batch size pas trop grande sinon Colab crash (ou votre PC). De même, la dimension de l'espace latent peut être ajustée, ici vu que les chats c'est pas si simple, environ 100 c'est bien.\n",
        "\n",
        "Pour charger le dataset, on utilise la méthode `image_dataset_from_directory` qui comme son nom l'indique prend juste le path du dossier d'images et en fait un dataset tensorflow que l'on peut mélanger, batcher...etc\n",
        "\n",
        "Les images ont leurs pixels entre 0 et 255 qu'on va renormaliser entre -1 et 1, ce qui est plus adapté pour les réseaux de neurones et bien pour des GAN car on a une moyenne nulle.\n",
        "On va aussi les afficher parce que c'est bien de savoir sur quoi on travaille quand même.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFtB5YCWO2Pm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "IMG_SHAPE = (64,64,3)\n",
        "\n",
        "x_train = keras.utils.image_dataset_from_directory('dataset',image_size=(64,64),batch_size=BATCH_SIZE,shuffle=True,seed=123,labels=None)\n",
        "\n",
        "#Normalisation des données\n",
        "x_train = x_train.map(lambda x: (x-127.5)/127.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "ZF6nSfKQP-IM",
        "outputId": "cba7fa66-39ee-488b-875f-78018e0ea2a1"
      },
      "outputs": [],
      "source": [
        "test_batch = next(iter(x_train))\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "for i in range(25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.axis('off')\n",
        "  #On oublie pas de faire image * 0.5 + 0.5 pour revenir dans [0,1]\n",
        "  plt.imshow(test_batch[i]*0.5+0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZZBKWamPNxO"
      },
      "source": [
        "## Le discriminateur\n",
        "\n",
        "On va faire le discriminateur.\n",
        "\n",
        "En entrée : la shape de l'image, typiquement (64,64,3) ici (c'est couleur pour rappel donc il y a bien 3 canaux).\n",
        "\n",
        "Pour vous aider, un petit rappel des blocs à mettre:\n",
        "- `Conv2D(filtres,kernel_size,strides,padding='same' ou 'valid')`\n",
        "\n",
        "  Typiquement, on utilise une taille de kernel de 3 et des filtres qui sont des puissances de 2 (32,64,128...).\n",
        "  \n",
        "  Pour rappel, le padding `'same'` équivaut à rajouter des zéros pour garder la même taille d'image en sortie (si les strides sont de 1) et avec `'valid'` on en rajoute pas. `'same'` est donc conseillé ici pour éviter les surprises (et de se torturer la tête sur la taille en sortie même si vous savez évidemment que c'est $\\frac{n+2p-f}{s}+1$).\n",
        "   \n",
        "   On ne met pas l'activation tout de suite car on ajoute d'abord de la batch normalization pour renormaliser le batch en 0 et ainsi profiter du comportement de la ReLU en ce point.\n",
        "\n",
        "  Pour rappel, on augmente le nombre de filtres au fur et à mesure de l'architecture dans le discriminateur.\n",
        "- `BatchNormalization()` (pas d'arguments, on laisse par défaut)\n",
        "- `LeakyReLU(alpha=0.2)` (pente de la leaky relu dans $]-\\infty,0]$)\n",
        "\n",
        "Mettez 3-4 blocs comme ça et faites des essais.\n",
        "\n",
        "Ici pas de `Pooling` car on réduit la taille des images directement avec du stride (typiquement 2 à chaque Convolution en comptant bien la taille qu'on obtient à la fin). \n",
        "\n",
        "En sortie on veut une dimension (1) grâce à une `Flatten` puis des `Dense` en oubliant pas la sigmoïde `activation='sigmoid'`à la fin. \n",
        "\n",
        "\n",
        "*Alternative*:\n",
        "- C'est aussi possible de bien calculer la taille de l'image pour terminer par une `Convolution` avec une sortie de dimension (1,1,1) puis une `Flatten` (un peu mieux).\n",
        "\n",
        "  **Exemple** : \n",
        "  - Image de (64,64,3) -> 4 Blocs de convolution avec du stride de 2 : $64/2^4$ -> Feature map de taille (4,4,nombre_de_filtres). \n",
        "\n",
        "    On peut ensuite finir avec une `Conv2D` avec une taille de filtre de 4 et du padding `'valid'` pour juste faire une combinaison de tous les pixels restants, ce qui nous donne bien notre unique pixel de dimension (1,1,1) sans oublier la sigmoïde et on peut ensuite `Flatten` tout ça).\n",
        "\n",
        "Pas besoin de compiler car on va faire un entraînement personnalisé après.\n",
        "\n",
        "<details> \n",
        "<summary>Antisèche</summary>\n",
        "Conv2D(32 filtres,kernel taille 3,stride 2,padding same) -> BN -> LR <br />  \n",
        "-> Conv2D(64,kernel 3,stride 2,padding same) -> BN -> LR <br />  \n",
        "-> Conv2D(128,3,2,same) -> BN -> LR <br />  \n",
        "-> Conv2D(256,3,2,same) -> BN -> LR <br />  \n",
        "-> Conv2D(1,4,1,valid) -> Sigmoïde -> Flatten ou Flatten -> Dense(1) -> Sigmoïde\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TasqSOiOk24X"
      },
      "outputs": [],
      "source": [
        "def define_discriminator(im_shape=(64,64,3)):\n",
        "  model = models.Sequential()\n",
        "  ###Ajouter les couches ici (rappel: on peut utiliser model.add(layers.CoucheX(arguments)))\n",
        "  model.add(layers.Conv2D(64,3,2,padding='same',input_shape=im_shape))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(128,3,2,padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(256,3,2,padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(512,3,2,padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Conv2D(1,4,1,padding='valid',activation='sigmoid'))\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  return model\n",
        "\n",
        "##Petit tips : faire un .summary() pour vérifier qu'on s'est pas trompé dans les dimensions de sortie\n",
        "define_discriminator().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaDZbie7Qw2D"
      },
      "source": [
        "## Le générateur\n",
        "\n",
        "On va faire le générateur.\n",
        "En entrée : un vecteur de l'espace latent de dimension `latent_dim`.\n",
        "\n",
        "`Dense(4 x 4 x 1024,input_shape=(latent_dim, ))`\n",
        "\n",
        "`Reshape((4,4,1024))`\n",
        "\n",
        "\n",
        "On peut aussi directement redimensionner le vecteur de l'espace latent sans la couche Dense.\n",
        "\n",
        "\n",
        "Pour vous aider, un petit rappel des blocs à mettre ensuite:\n",
        "- `Conv2DTranspose(filtres,kernel_size,strides,padding='same' ou 'valid')`\n",
        "\n",
        "  Pour rappel, on diminue le nombre de filtres au fur et à mesure de l'architecture dans le générateur (512,256,...)\n",
        "\n",
        "- `BatchNormalization()`\n",
        "- `ReLU()`\n",
        "\n",
        "Mettez 2-3 blocs comme ça et faites des essais aussi. Faites bien attention à la taille de vos images tout au long de l'architecture pour bien avoir la taille d'image finale voulue.\n",
        "\n",
        "**Exemple**:\n",
        "\n",
        "- On a transformé notre vecteur latent en image de dimension (4,4,1024). Pour avoir du (64,64,3), il faut donc 4 blocs de `Conv2DTranspose` (avec padding) pour avoir $4*2^4=64$. Libre à vous de changer l'entrée pour mettre le nombre de blocs que vous voulez.\n",
        "\n",
        "\n",
        "Ici aussi pas de `UpSampling2D` car on augmente la taille avec du stride (2 aussi souvent) à chaque convolution transposée.\n",
        "\n",
        "En sortie on veut une dimension (64,64,3) grâce à une `Conv2D` avec 3 filtres, on oublie pas de prendre une activation en tangente hyperbolique `'tanh'` pour avoir des pixels dans [-1,1].\n",
        "\n",
        "<details>\n",
        "<summary>Antisèche</summary>\n",
        "  Dense(4*4*1024 neurones) -> Reshape(en (4,4,1024))\n",
        "  -> ConvTransposée(256 filtres,kernel taille 3,stride 2,padding same) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(128,kernel 3,stride 2,padding same) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(64,3,2,same) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(32,3,2,same) -> BN -> ReLU <br />  \n",
        "  -> ConvTransposée(3,3,1,same) -> Tanh\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHCUgA6lKuwj"
      },
      "outputs": [],
      "source": [
        "def define_generator(latent_dim=LATENT_DIM):\n",
        "\tmodel = models.Sequential()\n",
        "\t###Ajouter les couches ici\n",
        "\tmodel.add(layers.Dense(4*4*1024,input_shape=(latent_dim,)))\n",
        "\tmodel.add(layers.Reshape((4,4,1024)))\n",
        "\tmodel.add(layers.Conv2DTranspose(256,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2DTranspose(128,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2DTranspose(64,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2DTranspose(32,3,2,padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(3,3,1,padding='same',activation='tanh'))\n",
        "\n",
        "\treturn model\n",
        "\n",
        "##Petit tips : faire un .summary() pour vérifier qu'on s'est pas trompé dans les dimensions de sortie\n",
        "define_generator().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Fbk2BnWJOO"
      },
      "source": [
        "## Le train step\n",
        "\n",
        "Cette partie est assez importante car elle permet de comprendre comment on entraîne vraiment un GAN, c'est-à-dire à quoi on compare les sorties pour entraîner correctement le discriminateur et le générateur. Ici, pas de `.fit` malheureusement.\n",
        "On va définir un `train_step`, c'est à dire ce qu'on va faire comme opérations à chaque batch :\n",
        "\n",
        "Entraîner le discriminateur :\n",
        "\n",
        "- Générer des images fausses à partir de bruit gaussien et en prédire les labels : on a besoin ici d'un vecteur latent de dimension `(batch_size,latent_dim)`. Ensuite, on fait passer ce bruit dans le générateur pour obtenir des fausses images. Enfin, on récupère la sortie du discriminateur sur celles-ci.\n",
        "\n",
        "\n",
        "```\n",
        "my_latent_vector = tf.random.normal(shape)\n",
        "fake_images = generator(...)\n",
        "fake_predictions = discriminator(...)\n",
        "```\n",
        "\n",
        "\n",
        "- Prendre des images vraies du dataset et en prédire aussi les labels. Donc la sortie du discriminateur sur les vraies images.\n",
        "\n",
        "\n",
        "```\n",
        "real_predictions = ...\n",
        "```\n",
        "\n",
        "\n",
        "- Calculer la loss en comparant les prédictions sur les fausses avec des 0 et les prédictions sur les vraies avec des 1.\n",
        "\n",
        "\n",
        "\n",
        "La binary crossentropy prend en argument les labels visés puis ceux prédits.\n",
        "Pour avoir des 1 ou des 0: `tf.ones(shape)` ou `tf.zeros(shape)` avec shape = (batch_size,1). Dans l'exemple suivant, cela corresponds aux `y_true` et les prédictions précédentes aux `y_pred`.\n",
        "```\n",
        "discriminator_loss_on_real = loss(y_true1,y_pred1)\n",
        "discriminator_loss_on_fake = loss(y_true2,y_pred2)\n",
        "discriminator_loss = discriminator_loss_on_real + discriminator_loss_on_fake\n",
        "```\n",
        "\n",
        "\n",
        "- Calculer les gradients en fonction de la loss calculée et les différents paramètres du modèle\n",
        "- On applique les gradients calculés avec l'optimisateur choisi\n",
        "\n",
        "Entraîner le générateur (presque la même chose):\n",
        "- Générer des images fausses à partir de bruit gaussien et en prédire les labels.\n",
        "- Calculer la loss en comparant les prédictions sur les fausses avec des 1 (on veut tromper le discriminateur).\n",
        "- Calculer les gradients en fonction de la loss calculée et les différents paramètres du modèle\n",
        "- On applique les gradients calculés avec l'optimisateur choisi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moZPLFqPSnz9"
      },
      "outputs": [],
      "source": [
        "def train_step(real_images,generator,discriminator,loss,g_opt,d_opt):\n",
        "  batch_size = tf.shape(real_images)[0]\n",
        "  global LATENT_DIM\n",
        "  with tf.GradientTape() as disc_tape:\n",
        "\n",
        "    ###A compléter###\n",
        "    latent_vector = tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
        "    fake_images = generator(latent_vector)\n",
        "    #Ce sont les prédictions du discriminateur\n",
        "    real_predictions = discriminator(real_images)\n",
        "    fake_predictions = discriminator(fake_images)\n",
        "\n",
        "    #Les labels sont les vrais labels des images du dataset (0 ou 1)\n",
        "    real_labels = tf.ones(shape=(batch_size,1))\n",
        "    fake_labels = tf.zeros(shape=(batch_size,1))\n",
        "\n",
        "    disc_loss_on_real = loss(real_labels,real_predictions)\n",
        "    disc_loss_on_fake = loss(fake_labels,fake_predictions)\n",
        "    disc_loss = disc_loss_on_real + disc_loss_on_fake\n",
        "    ######\n",
        "\n",
        "  disc_grad = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
        "  d_opt.apply_gradients(zip(disc_grad,discriminator.trainable_variables))\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape:\n",
        "\n",
        "    ###A compléter###\n",
        "    latent_vector = tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
        "    generated_images = generator(latent_vector)\n",
        "    fake_predictions = discriminator(generated_images)\n",
        "\n",
        "    #Rappel : on veut comparer les images générées à des 1 pour tromper le discriminateur cette fois\n",
        "    real_labels = tf.ones(shape=(batch_size,1))\n",
        "    gen_loss = loss(real_labels,fake_predictions)\n",
        "    ######\n",
        "\n",
        "  gen_grad = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
        "  g_opt.apply_gradients(zip(gen_grad,generator.trainable_variables))\n",
        "  return gen_loss,disc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPUvdewb7Ak5"
      },
      "source": [
        "## Le train\n",
        "On code la fonction d'entraînement principale.\n",
        "Il reste à compléter la loss et les optimisateurs à utiliser, typiquement ici la `BinaryCrossentropy` et `Adam(learning_rate=2e-4,beta_1=0.5)`.\n",
        "\n",
        "Dans un premier temps, on peut prendre les mêmes optimiseurs pour les deux quitte à adapter pour tester après (changer le learning rate par exemple pour rééquilibrer un peu l'entraînement).\n",
        "\n",
        "Il aurait été possible de faire une boucle d'entraînement plus poussée ou d'utiliser d'autres méthodes de keras directement (suivre les tutoriels sur https://keras.io/guides/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOXC7WKmTI_Z"
      },
      "outputs": [],
      "source": [
        "def train(dataset,generator,discriminator,epochs,fixed_seed = tf.random.normal((25,LATENT_DIM)),seed=42):\n",
        "  \n",
        "  ###A compléter###\n",
        "  loss = keras.losses.BinaryCrossentropy()\n",
        "  g_opt = keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
        "  d_opt = keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
        "\n",
        "  Lgen_loss = []\n",
        "  Ldisc_loss = []\n",
        "  X = []\n",
        "  j = 0\n",
        "  ######\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    progress_bar = tqdm(dataset)\n",
        "    ##Vu que c'est un dataset tensorflow, on ne peut itérer directement dessus avec son indice. On va juste prendre à chaque fois batch par batch.\n",
        "    for _,image_batch in enumerate(progress_bar):\n",
        "        j += 1\n",
        "        gen_loss, disc_loss = train_step(image_batch,generator,discriminator,loss,g_opt,d_opt)\n",
        "        \n",
        "        X.append(j)\n",
        "        Lgen_loss.append(gen_loss)\n",
        "        Ldisc_loss.append(disc_loss)\n",
        "\n",
        "    clear_output(wait=False)\n",
        "    generate_and_save_plots(X, Lgen_loss, Ldisc_loss,  epoch+1)\n",
        "    summarize_performance(generator,fixed_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSDZildZALv"
      },
      "source": [
        "## L'affichage à chaque epoch\n",
        "\n",
        "Petite fonction qui affiche les images obtenues à chaque epoch. On va **afficher** 25 images avec la même seed (toujours du même vecteur latent) pour voir l'amélioration progressive de l'image. Ce n'est pas de l'overfitting sur un seul vecteur car on **entraîne** bien à partir de vecteurs différents à chaque fois avant.\n",
        "\n",
        "On affiche aussi après chaque epoch les courbes des loss du générateur et du discriminateur pour suivre l'entraînement (empêcher des vanishing gradients ou des exploding gradients)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D5j4ZlLXaOh"
      },
      "outputs": [],
      "source": [
        "def summarize_performance(generator,fixed_seed):\n",
        "  fake_images = generator.predict(fixed_seed)\n",
        "  fig = plt.figure(figsize=(12,12))\n",
        "  for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fake_images[i]*0.5+0.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDbwKnC1pLFN"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_plots(X, Lgen_loss, Ldisc_loss, epoch):\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    plt.plot(X,Lgen_loss, label = 'gen_loss')\n",
        "    plt.plot(X,Ldisc_loss, label = 'disc_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAx8pu5P7sOR"
      },
      "source": [
        "## Ici vous lancez tout!\n",
        "Choisissez le nombre d'epochs que vous voulez. Pour MNIST, quelques dizaines d'epochs sont suffisantes pour avoir des résultats mais vous pourrez toujours relancer si vous n'êtes pas satisfaits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "vKElP8cy8dil",
        "outputId": "4594d8f6-7c4c-40f7-9134-f054cd97a2af"
      },
      "outputs": [],
      "source": [
        "generator = define_generator(latent_dim=LATENT_DIM)\n",
        "discriminator = define_discriminator(im_shape=(64,64,3))\n",
        "EPOCHS = 10\n",
        "train(x_train,generator,discriminator,EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prn-zEwC7jhy"
      },
      "source": [
        "# A développer\n",
        "## Autres datasets\n",
        "\n",
        "Vous pouvez essayer de générer des images sur d'autres datasets comme Fashion MNIST, CIFAR, CelebA ou n'importe quel type d'images qui vous font plaisir. S'il faut le télécharger et l'importer sur Colab, vous pouvez directement monter votre Drive et uploader votre dataset sur ce dernier : \n",
        "\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "```\n",
        "\n",
        "## Autres architectures\n",
        "\n",
        "Vous pouvez rajouter des blocs dans le générateur ou le discriminateur, essayer d'ajouter du Dropout, enlever les biais dans les couches, modifier la dimension de l'espace latent, initialiser les poids d'une certaine façon...etc Si vous êtes déjà à jour sur les CNN avancés (ResNET, MobileNet, EfficientNet), vous pouvez essayer de faire un GAN sur ces bases, ce qui permettra de résoudre certains problèmes comme les vanishing gradient.\n",
        "\n",
        "## Tenter de forcer des problèmes\n",
        "\n",
        "Faites n'importe quoi ! \n",
        "\n",
        "Plus sérieusement, cela peut arriver surtout sur des datasets plus compliqués, la plupart des choses fonctionnent pour MNIST à part si on fait exprès de provoquer des problèmes.\n",
        "\n",
        "Vous pouvez essayer par exemple de déséquilibrer l'entraînement en modifiant les learning rate, en rajoutant beaucoup de couches seulement d'un côté...etc\n",
        "\n",
        "Ici on peut forcer le mode collapse en prenant une dimension latente de seulement 1. Le générateur aura alors plus de risques de générer des images sur un seul mode.\n",
        "\n",
        "## Tips d'entraînement possibles\n",
        "\n",
        "- Utiliser une loss différente, du genre la Wasserstein loss.\n",
        "\n",
        "- Changer le label visé pour les images vraies de 1 en 0.9\n",
        "\n",
        "- Rajouter du bruit sur les images\n",
        "\n",
        "- Ajouter des labels à l'entrée du générateur et du discriminateur (vous pourrez alors même choisir les classes des images générées)\n",
        "\n",
        "- Entraîner le discrminateur plus que le générateur (typiquement entre 3 à 5 boucles d'entraînement à chaque fois que le générateur en fait une).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM6h4yvial7i"
      },
      "source": [
        "# Ce qu'il faut retenir de ce TP\n",
        "- Architecture du générateur et du discriminateur :\n",
        "\n",
        "  `Conv2D(Transpose) -> BatchNorm -> ReLU ou LeakyReLU`\n",
        "\n",
        "  +Spécifités en entrée et sortie selon ce que l'on veut\n",
        "\n",
        "  Eviter les fully connected/denses si possible\n",
        "\n",
        "- Faire une boucle d'entraînement from scratch avec un train_step (très utile pour faire des choses plus compliquées)\n",
        "\n",
        "  `Loss -> gradient_tape.gradient() -> optimizer.apply_gradients`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TPGANatrous.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "32c9d2eacc2762f52bef3ce1121499e437c40c335720e1c9a43a308cd905b17a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
